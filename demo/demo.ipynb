{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "import wandb\n",
    "import torch\n",
    "from mp_transformer.models.transformer import MovementPrimitiveTransformer\n",
    "from mp_transformer.config import CONFIG\n",
    "from mp_transformer.train import setup\n",
    "from mp_transformer.utils import save_side_by_side_strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/daniel/git/mp-transformer\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path.cwd().parts[-1]\n",
    "if current_dir == \"demo\":\n",
    "    os.chdir(\"..\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/daniel/git/mp-transformer/wandb/run-20230614_164529-ns8n4kv9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tcs-mr/mp-transformer/runs/ns8n4kv9' target=\"_blank\">polar-planet-193</a></strong> to <a href='https://wandb.ai/tcs-mr/mp-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tcs-mr/mp-transformer' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tcs-mr/mp-transformer/runs/ns8n4kv9' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer/runs/ns8n4kv9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model:v2, 86.25MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"mp-transformer\")\n",
    "artifact = run.use_artifact(\"tcs-mr/mp-transformer/model:v2\", type='model')\n",
    "artifact_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./artifacts/model:v2\n"
     ]
    }
   ],
   "source": [
    "print(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-planet-193</strong> at: <a href='https://wandb.ai/tcs-mr/mp-transformer/runs/ns8n4kv9' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer/runs/ns8n4kv9</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230614_164529-ns8n4kv9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, train_dataset, val_dataset = setup(CONFIG)\n",
    "model = model.load_from_checkpoint(Path(artifact_dir, \"model.ckpt\"), config=CONFIG)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = val_dataset[0]\n",
    "poses, timestamps = item[\"poses\"], item[\"timestamps\"]\n",
    "# poses = torch.stack([poses[0, :], poses[-1, :]])\n",
    "# timestamps = torch.stack([timestamps[0], timestamps[-1]])\n",
    "# ys, timestamps = item[\"poses\"], item[\"timestamps\"]\n",
    "y_hat = model.infer(poses, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to tmp/comp_vid0.mp4\n",
      "Video saved to tmp/comp_vid1.mp4\n",
      "Video saved to tmp/comp_vid2.mp4\n",
      "Video saved to tmp/comp_vid3.mp4\n",
      "Video saved to tmp/comp_vid4.mp4\n",
      "Video saved to tmp/comp_vid5.mp4\n",
      "Moviepy - Building video tmp/comp_strip.mp4.\n",
      "Moviepy - Writing video tmp/comp_strip.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready tmp/comp_strip.mp4\n"
     ]
    }
   ],
   "source": [
    "save_side_by_side_strip(item, model, CONFIG[\"num_primitives\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"../tmp/comp_strip.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"../tmp/comp_strip.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_latents = torch.zeros(1, 6, 128)\n",
    "timestamps = timestamps.unsqueeze(0)\n",
    "sampled_latents = torch.randn_like(init_latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.decoder(timestamps, sampled_latents)\n",
    "recons_sequence = out[\"recons_sequence\"]\n",
    "recons_sequence = recons_sequence.squeeze(0).detach().numpy()\n",
    "recons_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from mp_transformer.datasets.toy_dataset import unnormalize_pose\n",
    "from mp_transformer.utils.generate_toy_data import BONE_LENGTHS, render_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for rec in recons_sequence:\n",
    "    rec = unnormalize_pose(rec)\n",
    "    img = render_image(rec, BONE_LENGTHS)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to tmp/gen_vid.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output_file = f\"tmp/gen_vid.mp4\"\n",
    "with imageio.get_writer(output_file, fps=10) as writer:\n",
    "    for img in imgs:\n",
    "        img_array = np.array(img)  # Convert PIL Image object to NumPy array\n",
    "        writer.append_data(img_array)\n",
    "\n",
    "print(f\"Video saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"../tmp/gen_vid.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"../tmp/gen_vid.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mp-transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
