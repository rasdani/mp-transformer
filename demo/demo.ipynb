{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "import wandb\n",
    "import torch\n",
    "from mp_transformer.models.transformer import MovementPrimitiveTransformer\n",
    "from mp_transformer.config import CONFIG\n",
    "from mp_transformer.train import setup\n",
    "from mp_transformer.utils import save_side_by_side_strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/daniel/git/mp-transformer\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path.cwd().parts[-1]\n",
    "if current_dir == \"demo\":\n",
    "    os.chdir(\"..\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-a\u001b[0m (\u001b[33mtcs-mr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/daniel/git/mp-transformer/wandb/run-20230619_132833-15kqoezk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tcs-mr/mp-transformer/runs/15kqoezk' target=\"_blank\">efficient-aardvark-213</a></strong> to <a href='https://wandb.ai/tcs-mr/mp-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tcs-mr/mp-transformer' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tcs-mr/mp-transformer/runs/15kqoezk' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer/runs/15kqoezk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model:v5, 86.25MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"mp-transformer\")\n",
    "artifact = run.use_artifact(\"tcs-mr/mp-transformer/model:v5\", type='model')\n",
    "artifact_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./artifacts/model:v5\n"
     ]
    }
   ],
   "source": [
    "print(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">efficient-aardvark-213</strong> at: <a href='https://wandb.ai/tcs-mr/mp-transformer/runs/15kqoezk' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer/runs/15kqoezk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230619_132833-15kqoezk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, train_dataset, val_dataset = setup(CONFIG)\n",
    "artifact_dir = \"./artifacts/model:v5\"\n",
    "model = model.load_from_checkpoint(Path(artifact_dir, \"model.ckpt\"), config=CONFIG)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = val_dataset[-1]\n",
    "poses, timestamps = item[\"poses\"], item[\"timestamps\"]\n",
    "# poses = torch.stack([poses[0, :], poses[-1, :]])\n",
    "# timestamps = torch.stack([timestamps[0], timestamps[-1]])\n",
    "# ys, timestamps = item[\"poses\"], item[\"timestamps\"]\n",
    "# poses[48:80, :] = 0\n",
    "# timestamps[48:80] = 0\n",
    "# poses[0:, :] = 0\n",
    "# timestamps[0:] = 0\n",
    "# item[\"poses\"] = poses\n",
    "# item[\"timestamps\"] = timestamps\n",
    "y_hat = model.infer(poses, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to tmp/comp_vid.mp4\n",
      "Video saved to tmp/comp_vid0.mp4\n",
      "Video saved to tmp/comp_vid1.mp4\n",
      "Video saved to tmp/comp_vid2.mp4\n",
      "Video saved to tmp/comp_vid3.mp4\n",
      "Video saved to tmp/comp_vid4.mp4\n",
      "Video saved to tmp/comp_vid5.mp4\n",
      "Moviepy - Building video tmp/comp_strip.mp4.\n",
      "Moviepy - Writing video tmp/comp_strip.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready tmp/comp_strip.mp4\n"
     ]
    }
   ],
   "source": [
    "# print(item[\"poses\"], item[\"timestamps\"])\n",
    "save_side_by_side_strip(item, model, CONFIG[\"num_primitives\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"../tmp/comp_strip.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"../tmp/comp_strip.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_latents = torch.zeros(1, 6, 128)\n",
    "timestamps = timestamps.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 3), numpy.ndarray)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_latents = torch.randn_like(init_latents)\n",
    "out = model.decoder(timestamps, sampled_latents)\n",
    "recons_sequence = out[\"recons_sequence\"]\n",
    "recons_sequence = recons_sequence.squeeze(0).detach().numpy()\n",
    "recons_sequence.shape, type(recons_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from mp_transformer.datasets.toy_dataset import unnormalize_pose\n",
    "from mp_transformer.utils.generate_toy_data import BONE_LENGTHS, render_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for rec in recons_sequence:\n",
    "    rec = unnormalize_pose(rec)\n",
    "    img = render_image(rec, BONE_LENGTHS)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to tmp/gen_vid.mp4\n"
     ]
    }
   ],
   "source": [
    "output_file = f\"tmp/gen_vid.mp4\"\n",
    "with imageio.get_writer(output_file, fps=20) as writer:\n",
    "    for img in imgs:\n",
    "        img_array = np.array(img)  # Convert PIL Image object to NumPy array\n",
    "        writer.append_data(img_array)\n",
    "\n",
    "print(f\"Video saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"../tmp/gen_vid.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"../tmp/gen_vid.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sequences(model, joint_angles_before, joint_angles_after, timestamps_before, timestamps_after, num_interpolations=10):\n",
    "    \"\"\"\n",
    "    Given a VAE-Transformer model, joint angles and timestamps before and after the missing part, interpolates between these sequences.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The VAE-Transformer model.\n",
    "        joint_angles_before (torch.Tensor): A tensor of joint angles before the missing part.\n",
    "        joint_angles_after (torch.Tensor): A tensor of joint angles after the missing part.\n",
    "        timestamps_before (torch.Tensor): A tensor of timestamps before the missing part.\n",
    "        timestamps_after (torch.Tensor): A tensor of timestamps after the missing part.\n",
    "        num_interpolations (int): The number of interpolation steps.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of interpolated sequences.\n",
    "    \"\"\"\n",
    "    joint_angles_before, joint_angles_after = joint_angles_before.unsqueeze(0), joint_angles_after.unsqueeze(0)\n",
    "    timestamps_before, timestamps_after = timestamps_before.unsqueeze(0), timestamps_after.unsqueeze(0)\n",
    "    print(joint_angles_before.shape, joint_angles_after.shape, timestamps_before.shape, timestamps_after.shape)\n",
    "    # Encode the sequences before and after the missing part\n",
    "    encoder_outputs_before = model.encoder(joint_angles_before, timestamps_before)\n",
    "    encoder_outputs_after = model.encoder(joint_angles_after, timestamps_after)\n",
    "\n",
    "    # Get the latent primitives\n",
    "    latents_before = encoder_outputs_before[\"latent_primitives\"]\n",
    "    # TODO: latents_middle !!!\n",
    "    # TODO: or single latents and sample middle part before\n",
    "    # TODO: or average latents_before and latents_after, since time is encoded implicitly\n",
    "    latents_after = encoder_outputs_after[\"latent_primitives\"]\n",
    "    print(f\"{latents_before.shape=}, {latents_after.shape=}\")\n",
    "\n",
    "    interpolated_sequences = []\n",
    "\n",
    "    timestamps = torch.linspace(0, 1, 128).unsqueeze(0)\n",
    "    # TODO: How to interpolate between the latent primitives?\n",
    "    for i in range(num_interpolations):\n",
    "        alpha = i / (num_interpolations - 1)  # linear interpolation coefficient\n",
    "        interpolated_latents = alpha * latents_before + (1 - alpha) * latents_after\n",
    "\n",
    "        # Decode the interpolated latents\n",
    "        decoder_outputs = model.decoder(timestamps, interpolated_latents)  # assuming timestamps_before and timestamps_after are the same\n",
    "\n",
    "        # Get the reconstructed sequence\n",
    "        recons_sequence = decoder_outputs[\"recons_sequence\"]\n",
    "\n",
    "        interpolated_sequences.append(recons_sequence)\n",
    "\n",
    "    return interpolated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59, 3]), torch.Size([58, 3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses_before = poses[:59, :]\n",
    "poses_after = poses[70:, :]\n",
    "poses_before.shape, poses_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59]), torch.Size([58]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps = item[\"timestamps\"]\n",
    "timestamps_before = timestamps[:59]\n",
    "timestamps_after = timestamps[70:]\n",
    "timestamps_before.shape, timestamps_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 59, 3]) torch.Size([1, 58, 3]) torch.Size([1, 59]) torch.Size([1, 58])\n",
      "pose_embeddings.shape=torch.Size([1, 59, 128])\n",
      "self.positional_encoding(timestamps).shape=torch.Size([1, 59, 128])\n",
      "pose_embeddings.shape=torch.Size([1, 58, 128])\n",
      "self.positional_encoding(timestamps).shape=torch.Size([1, 58, 128])\n",
      "latents_before.shape=torch.Size([1, 6, 128]), latents_after.shape=torch.Size([1, 6, 128])\n"
     ]
    }
   ],
   "source": [
    "interpolate_sequences = interpolate_sequences(model, poses_before, poses_after, timestamps_before, timestamps_after, num_interpolations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 128, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(interpolate_sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Number of angles and bone_lengths should be the same but: 1 is not 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/data/daniel/git/mp-transformer/mp_transformer/utils/generate_toy_data.py:78\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(angles, bone_lenghts)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(angles) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(bone_lengths)\n\u001b[1;32m     79\u001b[0m     \u001b[39m# assert angles.shape == bone_lengths.shape\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m rec \u001b[39min\u001b[39;00m interpolate_sequences:\n\u001b[1;32m      3\u001b[0m     rec \u001b[39m=\u001b[39m unnormalize_pose(rec)\n\u001b[0;32m----> 4\u001b[0m     img \u001b[39m=\u001b[39m render_image(rec, BONE_LENGTHS)\n\u001b[1;32m      5\u001b[0m     imgs\u001b[39m.\u001b[39mappend(img)\n",
      "File \u001b[0;32m/data/daniel/git/mp-transformer/mp_transformer/utils/generate_toy_data.py:129\u001b[0m, in \u001b[0;36mrender_image\u001b[0;34m(pose, bone_lengths)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender_image\u001b[39m(pose, bone_lengths\u001b[39m=\u001b[39mBONE_LENGTHS):\n\u001b[0;32m--> 129\u001b[0m     coordinates \u001b[39m=\u001b[39m forward(pose, bone_lengths)\n\u001b[1;32m    130\u001b[0m     img \u001b[39m=\u001b[39m coordinates_to_image(coordinates)\n\u001b[1;32m    132\u001b[0m     \u001b[39m# Apply a colormap (viridis) to the image\u001b[39;00m\n",
      "File \u001b[0;32m/data/daniel/git/mp-transformer/mp_transformer/utils/generate_toy_data.py:81\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(angles, bone_lenghts)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[39m# assert angles.shape == bone_lengths.shape\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m excp:\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of angles and bone_lengths should be the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(angles)\u001b[39m}\u001b[39;00m\u001b[39m is not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(bone_lengths)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m             \u001b[39m# f\" but: {angles.shape} is not {bone_lengths.shape}\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39mexcp\u001b[39;00m\n\u001b[1;32m     87\u001b[0m coordinates \u001b[39m=\u001b[39m [(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)]\n\u001b[1;32m     88\u001b[0m cumulative_angle \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mException\u001b[0m: Number of angles and bone_lengths should be the same but: 1 is not 3"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "for rec in interpolate_sequences:\n",
    "    rec = unnormalize_pose(rec)\n",
    "    img = render_image(rec, BONE_LENGTHS)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0711, 0.6657, 0.3384],\n",
       "         [0.0488, 0.6581, 0.3320],\n",
       "         [0.0276, 0.6545, 0.3330],\n",
       "         [0.0076, 0.6548, 0.3411],\n",
       "         [0.9891, 0.6581, 0.3555],\n",
       "         [0.9723, 0.6630, 0.3746],\n",
       "         [0.9573, 0.6678, 0.3969],\n",
       "         [0.9443, 0.6716, 0.4202],\n",
       "         [0.9336, 0.6740, 0.4424],\n",
       "         [0.9252, 0.6754, 0.4617],\n",
       "         [0.9193, 0.6766, 0.4765],\n",
       "         [0.9161, 0.6783, 0.4862],\n",
       "         [0.9154, 0.6811, 0.4908],\n",
       "         [0.9175, 0.6846, 0.4914],\n",
       "         [0.9224, 0.6880, 0.4894],\n",
       "         [0.9300, 0.6899, 0.4863],\n",
       "         [0.9402, 0.6892, 0.4833],\n",
       "         [0.9531, 0.6850, 0.4810],\n",
       "         [0.9685, 0.6773, 0.4790],\n",
       "         [0.9862, 0.6671, 0.4766],\n",
       "         [0.0062, 0.6554, 0.4725],\n",
       "         [0.0281, 0.6435, 0.4659],\n",
       "         [0.0518, 0.6322, 0.4564],\n",
       "         [0.0771, 0.6213, 0.4442],\n",
       "         [0.1036, 0.6102, 0.4303],\n",
       "         [0.1312, 0.5979, 0.4158],\n",
       "         [0.1595, 0.5833, 0.4019],\n",
       "         [0.1883, 0.5663, 0.3897],\n",
       "         [0.2173, 0.5473, 0.3800],\n",
       "         [0.2462, 0.5277, 0.3732],\n",
       "         [0.2749, 0.5096, 0.3695],\n",
       "         [0.3029, 0.4953, 0.3692],\n",
       "         [0.3303, 0.4869, 0.3724],\n",
       "         [0.3566, 0.4861, 0.3792],\n",
       "         [0.3819, 0.4935, 0.3894],\n",
       "         [0.4059, 0.5087, 0.4026],\n",
       "         [0.4286, 0.5301, 0.4178],\n",
       "         [0.4499, 0.5555, 0.4337],\n",
       "         [0.4698, 0.5822, 0.4485],\n",
       "         [0.4883, 0.6076, 0.4608],\n",
       "         [0.5054, 0.6296, 0.4691],\n",
       "         [0.5212, 0.6473, 0.4731],\n",
       "         [0.5358, 0.6605, 0.4730],\n",
       "         [0.5494, 0.6702, 0.4701],\n",
       "         [0.5621, 0.6777, 0.4659],\n",
       "         [0.5741, 0.6845, 0.4622],\n",
       "         [0.5856, 0.6919, 0.4604],\n",
       "         [0.5969, 0.7004, 0.4611],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000],\n",
       "         [0.3282, 0.3869, 0.4471],\n",
       "         [0.3356, 0.3593, 0.4626],\n",
       "         [0.3397, 0.3259, 0.4818],\n",
       "         [0.3403, 0.2918, 0.5028],\n",
       "         [0.3374, 0.2630, 0.5237],\n",
       "         [0.3311, 0.2455, 0.5426],\n",
       "         [0.3214, 0.2441, 0.5577],\n",
       "         [0.3083, 0.2615, 0.5678],\n",
       "         [0.2920, 0.2981, 0.5725],\n",
       "         [0.2725, 0.3514, 0.5720],\n",
       "         [0.2502, 0.4169, 0.5677],\n",
       "         [0.2250, 0.4882, 0.5617],\n",
       "         [0.1973, 0.5585, 0.5566],\n",
       "         [0.1673, 0.6215, 0.5550],\n",
       "         [0.1352, 0.6724, 0.5589],\n",
       "         [0.1013, 0.7082, 0.5692],\n",
       "         [0.0658, 0.7285, 0.5859],\n",
       "         [0.0291, 0.7347, 0.6072],\n",
       "         [0.9915, 0.7298, 0.6309],\n",
       "         [0.9531, 0.7176, 0.6539],\n",
       "         [0.9144, 0.7022, 0.6733],\n",
       "         [0.8756, 0.6870, 0.6870],\n",
       "         [0.8369, 0.6748, 0.6935],\n",
       "         [0.7987, 0.6675, 0.6927],\n",
       "         [0.7612, 0.6657, 0.6855],\n",
       "         [0.7246, 0.6693, 0.6735],\n",
       "         [0.6891, 0.6772, 0.6592],\n",
       "         [0.6551, 0.6877, 0.6453],\n",
       "         [0.6225, 0.6986, 0.6342],\n",
       "         [0.5918, 0.7075, 0.6278],\n",
       "         [0.5628, 0.7124, 0.6271],\n",
       "         [0.5359, 0.7122, 0.6321],\n",
       "         [0.5110, 0.7065, 0.6415],\n",
       "         [0.4883, 0.6963, 0.6531],\n",
       "         [0.4679, 0.6830, 0.6640],\n",
       "         [0.4496, 0.6686, 0.6716],\n",
       "         [0.4337, 0.6550, 0.6737],\n",
       "         [0.4199, 0.6434, 0.6693],\n",
       "         [0.4083, 0.6339, 0.6589],\n",
       "         [0.3989, 0.6260, 0.6443],\n",
       "         [0.3915, 0.6183, 0.6279],\n",
       "         [0.3861, 0.6090, 0.6123],\n",
       "         [0.3824, 0.5967, 0.5998],\n",
       "         [0.3805, 0.5804, 0.5917],\n",
       "         [0.3802, 0.5596, 0.5881],\n",
       "         [0.3813, 0.5347, 0.5887],\n",
       "         [0.3836, 0.5064, 0.5928],\n",
       "         [0.3871, 0.4755, 0.5997]]),\n",
       " tensor([0.0000, 0.0079, 0.0157, 0.0236, 0.0315, 0.0394, 0.0472, 0.0551, 0.0630,\n",
       "         0.0709, 0.0787, 0.0866, 0.0945, 0.1024, 0.1102, 0.1181, 0.1260, 0.1339,\n",
       "         0.1417, 0.1496, 0.1575, 0.1654, 0.1732, 0.1811, 0.1890, 0.1969, 0.2047,\n",
       "         0.2126, 0.2205, 0.2283, 0.2362, 0.2441, 0.2520, 0.2598, 0.2677, 0.2756,\n",
       "         0.2835, 0.2913, 0.2992, 0.3071, 0.3150, 0.3228, 0.3307, 0.3386, 0.3465,\n",
       "         0.3543, 0.3622, 0.3701, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6299,\n",
       "         0.6378, 0.6457, 0.6535, 0.6614, 0.6693, 0.6772, 0.6850, 0.6929, 0.7008,\n",
       "         0.7087, 0.7165, 0.7244, 0.7323, 0.7402, 0.7480, 0.7559, 0.7638, 0.7717,\n",
       "         0.7795, 0.7874, 0.7953, 0.8031, 0.8110, 0.8189, 0.8268, 0.8346, 0.8425,\n",
       "         0.8504, 0.8583, 0.8661, 0.8740, 0.8819, 0.8898, 0.8976, 0.9055, 0.9134,\n",
       "         0.9213, 0.9291, 0.9370, 0.9449, 0.9528, 0.9606, 0.9685, 0.9764, 0.9843,\n",
       "         0.9921, 1.0000]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses, timestamps = item[\"poses\"], item[\"timestamps\"]\n",
    "poses[48:80, :] = 0\n",
    "timestamps[48:80] = 0\n",
    "poses, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamps=tensor([[0.0000, 0.0079, 0.0157, 0.0236, 0.0315, 0.0394, 0.0472, 0.0551, 0.0630,\n",
      "         0.0709, 0.0787, 0.0866, 0.0945, 0.1024, 0.1102, 0.1181, 0.1260, 0.1339,\n",
      "         0.1417, 0.1496, 0.1575, 0.1654, 0.1732, 0.1811, 0.1890, 0.1969, 0.2047,\n",
      "         0.2126, 0.2205, 0.2283, 0.2362, 0.2441, 0.2520, 0.2598, 0.2677, 0.2756,\n",
      "         0.2835, 0.2913, 0.2992, 0.3071, 0.3150, 0.3228, 0.3307, 0.3386, 0.3465,\n",
      "         0.3543, 0.3622, 0.3701, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6299,\n",
      "         0.6378, 0.6457, 0.6535, 0.6614, 0.6693, 0.6772, 0.6850, 0.6929, 0.7008,\n",
      "         0.7087, 0.7165, 0.7244, 0.7323, 0.7402, 0.7480, 0.7559, 0.7638, 0.7717,\n",
      "         0.7795, 0.7874, 0.7953, 0.8031, 0.8110, 0.8189, 0.8268, 0.8346, 0.8425,\n",
      "         0.8504, 0.8583, 0.8661, 0.8740, 0.8819, 0.8898, 0.8976, 0.9055, 0.9134,\n",
      "         0.9213, 0.9291, 0.9370, 0.9449, 0.9528, 0.9606, 0.9685, 0.9764, 0.9843,\n",
      "         0.9921, 1.0000]])\n",
      "pose_embeddings.shape=torch.Size([1, 128, 128])\n",
      "self.positional_encoding(timestamps).shape=torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# poses, timestamps = poses.unsqueeze(0), timestamps.unsqueeze(0)\n",
    "y_hat = model.infer(poses, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
