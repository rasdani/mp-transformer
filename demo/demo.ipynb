{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "import wandb\n",
    "import torch\n",
    "from mp_transformer.models.transformer import MovementPrimitiveTransformer\n",
    "from mp_transformer.config import CONFIG\n",
    "from mp_transformer.train import setup\n",
    "from mp_transformer.utils import save_side_by_side_strip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/daniel/git/mp-transformer\n"
     ]
    }
   ],
   "source": [
    "current_dir = Path.cwd().parts[-1]\n",
    "if current_dir == \"demo\":\n",
    "    os.chdir(\"..\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniel-a\u001b[0m (\u001b[33mtcs-mr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/daniel/git/mp-transformer/wandb/run-20230617_161225-nm93aa1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tcs-mr/mp-transformer/runs/nm93aa1y' target=\"_blank\">atomic-vortex-207</a></strong> to <a href='https://wandb.ai/tcs-mr/mp-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tcs-mr/mp-transformer' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tcs-mr/mp-transformer/runs/nm93aa1y' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer/runs/nm93aa1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact model:v4, 86.25MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.1\n"
     ]
    }
   ],
   "source": [
    "run = wandb.init(project=\"mp-transformer\")\n",
    "artifact = run.use_artifact(\"tcs-mr/mp-transformer/model:v4\", type='model')\n",
    "artifact_dir = artifact.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./artifacts/model:v4\n"
     ]
    }
   ],
   "source": [
    "print(artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">atomic-vortex-207</strong> at: <a href='https://wandb.ai/tcs-mr/mp-transformer/runs/nm93aa1y' target=\"_blank\">https://wandb.ai/tcs-mr/mp-transformer/runs/nm93aa1y</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230617_161225-nm93aa1y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, train_dataset, val_dataset = setup(CONFIG)\n",
    "model = model.load_from_checkpoint(Path(artifact_dir, \"model.ckpt\"), config=CONFIG)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 3])\n",
      "torch.Size([128])\n",
      "timestamps=tensor([[0.0000, 0.0079, 0.0157, 0.0236, 0.0315, 0.0394, 0.0472, 0.0551, 0.0630,\n",
      "         0.0709, 0.0787, 0.0866, 0.0945, 0.1024, 0.1102, 0.1181, 0.1260, 0.1339,\n",
      "         0.1417, 0.1496, 0.1575, 0.1654, 0.1732, 0.1811, 0.1890, 0.1969, 0.2047,\n",
      "         0.2126, 0.2205, 0.2283, 0.2362, 0.2441, 0.2520, 0.2598, 0.2677, 0.2756,\n",
      "         0.2835, 0.2913, 0.2992, 0.3071, 0.3150, 0.3228, 0.3307, 0.3386, 0.3465,\n",
      "         0.3543, 0.3622, 0.3701, 0.3780, 0.3858, 0.3937, 0.4016, 0.4094, 0.4173,\n",
      "         0.4252, 0.4331, 0.4409, 0.4488, 0.4567, 0.4646, 0.4724, 0.4803, 0.4882,\n",
      "         0.4961, 0.5039, 0.5118, 0.5197, 0.5276, 0.5354, 0.5433, 0.5512, 0.5591,\n",
      "         0.5669, 0.5748, 0.5827, 0.5906, 0.5984, 0.6063, 0.6142, 0.6220, 0.6299,\n",
      "         0.6378, 0.6457, 0.6535, 0.6614, 0.6693, 0.6772, 0.6850, 0.6929, 0.7008,\n",
      "         0.7087, 0.7165, 0.7244, 0.7323, 0.7402, 0.7480, 0.7559, 0.7638, 0.7717,\n",
      "         0.7795, 0.7874, 0.7953, 0.8031, 0.8110, 0.8189, 0.8268, 0.8346, 0.8425,\n",
      "         0.8504, 0.8583, 0.8661, 0.8740, 0.8819, 0.8898, 0.8976, 0.9055, 0.9134,\n",
      "         0.9213, 0.9291, 0.9370, 0.9449, 0.9528, 0.9606, 0.9685, 0.9764, 0.9843,\n",
      "         0.9921, 1.0000]])\n",
      "pose_embeddings.shape=torch.Size([1, 128, 128])\n",
      "self.positional_encoding(timestamps).shape=torch.Size([1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "item = val_dataset[0]\n",
    "poses, timestamps = item[\"poses\"], item[\"timestamps\"]\n",
    "# poses = torch.stack([poses[0, :], poses[-1, :]])\n",
    "# timestamps = torch.stack([timestamps[0], timestamps[-1]])\n",
    "# ys, timestamps = item[\"poses\"], item[\"timestamps\"]\n",
    "print(poses.shape)\n",
    "print(timestamps.shape)\n",
    "y_hat = model.infer(poses, timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to tmp/comp_vid0.mp4\n",
      "Video saved to tmp/comp_vid1.mp4\n",
      "Video saved to tmp/comp_vid2.mp4\n",
      "Video saved to tmp/comp_vid3.mp4\n",
      "Video saved to tmp/comp_vid4.mp4\n",
      "Video saved to tmp/comp_vid5.mp4\n",
      "Moviepy - Building video tmp/comp_strip.mp4.\n",
      "Moviepy - Writing video tmp/comp_strip.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready tmp/comp_strip.mp4\n"
     ]
    }
   ],
   "source": [
    "save_side_by_side_strip(item, model, CONFIG[\"num_primitives\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"../tmp/comp_strip.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"../tmp/comp_strip.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_latents = torch.zeros(1, 6, 128)\n",
    "timestamps = timestamps.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 3), numpy.ndarray)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_latents = torch.randn_like(init_latents)\n",
    "out = model.decoder(timestamps, sampled_latents)\n",
    "recons_sequence = out[\"recons_sequence\"]\n",
    "recons_sequence = recons_sequence.squeeze(0).detach().numpy()\n",
    "recons_sequence.shape, type(recons_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "from mp_transformer.datasets.toy_dataset import unnormalize_pose\n",
    "from mp_transformer.utils.generate_toy_data import BONE_LENGTHS, render_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for rec in recons_sequence:\n",
    "    rec = unnormalize_pose(rec)\n",
    "    img = render_image(rec, BONE_LENGTHS)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to tmp/gen_vid.mp4\n"
     ]
    }
   ],
   "source": [
    "output_file = f\"tmp/gen_vid.mp4\"\n",
    "with imageio.get_writer(output_file, fps=20) as writer:\n",
    "    for img in imgs:\n",
    "        img_array = np.array(img)  # Convert PIL Image object to NumPy array\n",
    "        writer.append_data(img_array)\n",
    "\n",
    "print(f\"Video saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"../tmp/gen_vid.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"../tmp/gen_vid.mp4\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_sequences(model, joint_angles_before, joint_angles_after, timestamps_before, timestamps_after, num_interpolations=10):\n",
    "    \"\"\"\n",
    "    Given a VAE-Transformer model, joint angles and timestamps before and after the missing part, interpolates between these sequences.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The VAE-Transformer model.\n",
    "        joint_angles_before (torch.Tensor): A tensor of joint angles before the missing part.\n",
    "        joint_angles_after (torch.Tensor): A tensor of joint angles after the missing part.\n",
    "        timestamps_before (torch.Tensor): A tensor of timestamps before the missing part.\n",
    "        timestamps_after (torch.Tensor): A tensor of timestamps after the missing part.\n",
    "        num_interpolations (int): The number of interpolation steps.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of interpolated sequences.\n",
    "    \"\"\"\n",
    "    joint_angles_before, joint_angles_after = joint_angles_before.unsqueeze(0), joint_angles_after.unsqueeze(0)\n",
    "    timestamps_before, timestamps_after = timestamps_before.unsqueeze(0), timestamps_after.unsqueeze(0)\n",
    "    print(joint_angles_before.shape, joint_angles_after.shape, timestamps_before.shape, timestamps_after.shape)\n",
    "    # Encode the sequences before and after the missing part\n",
    "    encoder_outputs_before = model.encoder(joint_angles_before, timestamps_before)\n",
    "    encoder_outputs_after = model.encoder(joint_angles_after, timestamps_after)\n",
    "\n",
    "    # Get the latent primitives\n",
    "    latents_before = encoder_outputs_before[\"latent_primitives\"]\n",
    "    # TODO: latents_middle !!!\n",
    "    # TODO: or single latents and sample middle part before\n",
    "    # TODO: or average latents_before and latents_after, since time is encoded implicitly\n",
    "    latents_after = encoder_outputs_after[\"latent_primitives\"]\n",
    "    print(f\"{latents_before.shape=}, {latents_after.shape=}\")\n",
    "\n",
    "    interpolated_sequences = []\n",
    "\n",
    "    timestamps = torch.linspace(0, 1, 128).unsqueeze(0)\n",
    "    # TODO: How to interpolate between the latent primitives?\n",
    "    for i in range(num_interpolations):\n",
    "        alpha = i / (num_interpolations - 1)  # linear interpolation coefficient\n",
    "        interpolated_latents = alpha * latents_before + (1 - alpha) * latents_after\n",
    "\n",
    "        # Decode the interpolated latents\n",
    "        decoder_outputs = model.decoder(timestamps, interpolated_latents)  # assuming timestamps_before and timestamps_after are the same\n",
    "\n",
    "        # Get the reconstructed sequence\n",
    "        recons_sequence = decoder_outputs[\"recons_sequence\"]\n",
    "\n",
    "        interpolated_sequences.append(recons_sequence)\n",
    "\n",
    "    return interpolated_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59, 3]), torch.Size([58, 3]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses_before = poses[:59, :]\n",
    "poses_after = poses[70:, :]\n",
    "poses_before.shape, poses_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([59]), torch.Size([58]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamps = item[\"timestamps\"]\n",
    "timestamps_before = timestamps[:59]\n",
    "timestamps_after = timestamps[70:]\n",
    "timestamps_before.shape, timestamps_after.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 59, 3]) torch.Size([1, 58, 3]) torch.Size([1, 59]) torch.Size([1, 58])\n",
      "pose_embeddings.shape=torch.Size([1, 59, 128])\n",
      "self.positional_encoding(timestamps).shape=torch.Size([1, 59, 128])\n",
      "pose_embeddings.shape=torch.Size([1, 58, 128])\n",
      "self.positional_encoding(timestamps).shape=torch.Size([1, 58, 128])\n",
      "latents_before.shape=torch.Size([1, 6, 128]), latents_after.shape=torch.Size([1, 6, 128])\n"
     ]
    }
   ],
   "source": [
    "interpolate_sequences = interpolate_sequences(model, poses_before, poses_after, timestamps_before, timestamps_after, num_interpolations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 128, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(interpolate_sequences).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Number of angles and bone_lengths should be the same but: 1 is not 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/data/daniel/git/mp-transformer/mp_transformer/utils/generate_toy_data.py:78\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(angles, bone_lenghts)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(angles) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(bone_lengths)\n\u001b[1;32m     79\u001b[0m     \u001b[39m# assert angles.shape == bone_lengths.shape\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m rec \u001b[39min\u001b[39;00m interpolate_sequences:\n\u001b[1;32m      3\u001b[0m     rec \u001b[39m=\u001b[39m unnormalize_pose(rec)\n\u001b[0;32m----> 4\u001b[0m     img \u001b[39m=\u001b[39m render_image(rec, BONE_LENGTHS)\n\u001b[1;32m      5\u001b[0m     imgs\u001b[39m.\u001b[39mappend(img)\n",
      "File \u001b[0;32m/data/daniel/git/mp-transformer/mp_transformer/utils/generate_toy_data.py:129\u001b[0m, in \u001b[0;36mrender_image\u001b[0;34m(pose, bone_lengths)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender_image\u001b[39m(pose, bone_lengths\u001b[39m=\u001b[39mBONE_LENGTHS):\n\u001b[0;32m--> 129\u001b[0m     coordinates \u001b[39m=\u001b[39m forward(pose, bone_lengths)\n\u001b[1;32m    130\u001b[0m     img \u001b[39m=\u001b[39m coordinates_to_image(coordinates)\n\u001b[1;32m    132\u001b[0m     \u001b[39m# Apply a colormap (viridis) to the image\u001b[39;00m\n",
      "File \u001b[0;32m/data/daniel/git/mp-transformer/mp_transformer/utils/generate_toy_data.py:81\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(angles, bone_lenghts)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[39m# assert angles.shape == bone_lengths.shape\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m excp:\n\u001b[0;32m---> 81\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[1;32m     82\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of angles and bone_lengths should be the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(angles)\u001b[39m}\u001b[39;00m\u001b[39m is not \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(bone_lengths)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m             \u001b[39m# f\" but: {angles.shape} is not {bone_lengths.shape}\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39mexcp\u001b[39;00m\n\u001b[1;32m     87\u001b[0m coordinates \u001b[39m=\u001b[39m [(\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)]\n\u001b[1;32m     88\u001b[0m cumulative_angle \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mException\u001b[0m: Number of angles and bone_lengths should be the same but: 1 is not 3"
     ]
    }
   ],
   "source": [
    "imgs = []\n",
    "for rec in interpolate_sequences:\n",
    "    rec = unnormalize_pose(rec)\n",
    "    img = render_image(rec, BONE_LENGTHS)\n",
    "    imgs.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
