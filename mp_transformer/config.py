# might move this to a yaml or json
CONFIG = {
    "pose_dim": 3,
    "num_attention_heads": 4,
    "num_transformer_layers": 4,
    "latent_dim": 64,
    # "latent_dim": 32,
    "num_primitives": 6,
    # "num_primitives": 8,
    # "hidden_dim": 128,
    "hidden_dim": 256,
    "learn_segmentation": True,
    # "masking_slope": 1,
    "masking_slope": 0.5,
    # "kl_weight": 1e-5,
    "kl_weight": 1e-3,
    # "kl_weight": 1e-4,
    "durations_weight": 1e-3,
    # "durations_weight": 1e-1,
    "lr": 1e-4,
    "batch_size": 8,
    "N_train": 200000,
    "N_val": 40000,
    "sequence_length": 128,
    # "epochs": 250,
    # "epochs": 3000,
    "epochs": 500,
}

# for hyperparameter tuning with wandb sweep
SWEEP_CONFIG = {
    "method": "random",
    "metric": {"name": "val_loss", "goal": "minimize"},
    "parameters": {
        "pose_dim": {"value": 3},
        "num_attention_heads": {"value": 4},
        "num_transformer_layers": {"value": 4},
        "latent_dim": {"values": [16, 32, 64]},
        "num_primitives": {"values": [2, 4, 6, 8]},
        "hidden_dim": {"values": [64, 128, 256]},
        "learn_segmentation": {"value": True},
        # "masking_slope": {"min": 0.0, "max": 1.0},
        "masking_slope": {"value": 1.0},
        # "kl_weight": {"min": 0.0, "max": 1.0},
        # "kl_weight": {"values": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.2, 0.5, 1.0]},
        "kl_weight": {"values": [1e-5, 1e-4, 1e-3, 1e-2]},
        # "durations_weight": {"min": 0.0, "max": 1.0},
        # "durations_weight": {"values": [1e-3, 1e-2, 1e-1, 0.2, 0.5, 1.0]},
        "durations_weight": {"values": [1e-3, 1e-2, 1e-1]},
        # "lr": {"values": [1e-5, 1e-4, 1e-3, 1e-2]},
        "lr": {"values": [1e-5, 1e-4, 1e-3]},
        "batch_size": {"values": [8, 16, 32, 64, 128, 256]},
        "sequence_length": {"value": 128},
        "N_train": {"value": 200000},
        "N_val": {"value": 40000},
        "epochs": {"value": 100},
    },
    "early_terminate": {"type": "hyperband", "min_iter": 10, "max_iter": 100},
}
